---
layout: post
title:  "Massive Matrix Factorization for fMRI and Recommender Systems"
date:   2016-06-01 14:37:40 +0200
categories: research
---
{% include mathjax.html %}
In recent years, publicly available data have become larger and larger, both in signal size and number of samples. In many real-world domains, such data is used to train models that should remain *interpretable* and thus keep a reasonable complexity. In this regime, many classical machine-learning methods, both supervised and unsupervised are overwhelmed by the size of the data. Training cost and infrastructure needs become prohibitive for reasearchers. This has called for new innovative methods to reduce training complexity of models, while taking benefit of the new available information.

Let's take an example from the neuro-imaging community as this is the one I best know about. Today, the largest dataset publicly available dataset is **40TB large**: it includes rich recording from 900 subjects, and is expected to allows new breakthough in understanding the brain. Classical fMRI analysis has crucially relied on *matrix factorization* to reduce dimensionality of signals. Yet classical matrix factorization methods (PCA, ICA) are hardly appliable to the full HCP dataset, which has called for new ideas in this domain.

At out lab, we recently developed new methods to scale matrix factorization to terabyte scale data, which have yielded results in neuro-imagery and, rather unexpectedly, in recommender systems.

This post describes these methods with as little math as possible, and aims at giving intuitions on the challenges that arise from inrceasing data size. I will use the opportunity to make a quick review of the principles and methods for matrix factorization in data analysis, which is a key tool in many machine learning problems.

# Understanding data with matrix factorization

In unsupervised setting, machine learning methods aim at learning a model (namely, the parameters of a generative function) from a sequence of samples $(x_t)$ and prior knowledge of the distribution $x$. Typically, an interesting model should have a small number of parameters compared to the size of the data and the amount of prior knowledge injected in the model. We can learn something from data (and extends this knowledge to unseen data) if it is indeed *simpler* than its apparent dimension.

In matrix factorization frameworks, we model a sample as a linear combination of k *reference* samples $d_1, \dots, d_k$. This is a way to learn a compressed representation of the training data. For any sample $x$, we assume that there exists $(\alpha_1, \dots, \alpha_k)$ such that
$$x = \alpha_1 d_1 + \cdots +\alpha_k d_k$$

The model has then $p k$ parameters, and the representation of the data in this model is of size the number of non-zero coefficients in $(\alpha_1, \dots, \alpha_k)$.

This representation is compressed in two widely studied cases:

  - $k << p$, meaning that the data is indeed *low-rank*
  - $k > p$ but $(\alpha_i)$ is *sparse*

In the first case, the data is represented with $k (p + n)$ parameters, which is always smaller than $p\,n$. In the second case, the representation is of size $k\,p + s\,n$, where $s$ is the mean number of nonzero loadings per sample. The representation is compressed if $n >> p$ and $s << k$, which is typically the case when looking at image patches for examples.

In this post, we will exclusively study the first case.

## Examples

  Let's introduce the two cases that we use in our experiments:
  - fMRI data
