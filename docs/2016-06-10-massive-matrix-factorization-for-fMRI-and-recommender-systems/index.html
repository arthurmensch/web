<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v9.1.4 <https://hydejack.com/>
--><head><title>Massive Matrix Factorization for fMRI and Recommender Systems | Arthur Mensch</title><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="Massive Matrix Factorization for fMRI and Recommender Systems" /><meta name="author" content="Arthur Mensch" /><meta property="og:locale" content="en" /><meta name="description" content="Before presenting it at ICML New York, I will give a quick overview of our work on efficient matrix factorization for very large datasets. Our focus was to scale matrix factorization techniques for functional MRI, a domain where data to decompose is now at terabyte scale. Along the way, we also designed a encouraging proof-of-concept experiment for collaborative filtering." /><meta property="og:description" content="Before presenting it at ICML New York, I will give a quick overview of our work on efficient matrix factorization for very large datasets. Our focus was to scale matrix factorization techniques for functional MRI, a domain where data to decompose is now at terabyte scale. Along the way, we also designed a encouraging proof-of-concept experiment for collaborative filtering." /><meta property="og:site_name" content="Arthur Mensch" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2016-06-10T10:19:34+02:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Massive Matrix Factorization for fMRI and Recommender Systems" /> <script type="application/ld+json"> {"url":"/2016-06-10-massive-matrix-factorization-for-fMRI-and-recommender-systems/","headline":"Massive Matrix Factorization for fMRI and Recommender Systems","dateModified":"2018-03-19T11:19:53+01:00","datePublished":"2016-06-10T10:19:34+02:00","author":{"@type":"Person","name":"Arthur Mensch"},"mainEntityOfPage":{"@type":"WebPage","@id":"/2016-06-10-massive-matrix-factorization-for-fMRI-and-recommender-systems/"},"description":"Before presenting it at ICML New York, I will give a quick overview of our work on efficient matrix factorization for very large datasets. Our focus was to scale matrix factorization techniques for functional MRI, a domain where data to decompose is now at terabyte scale. Along the way, we also designed a encouraging proof-of-concept experiment for collaborative filtering.","@type":"BlogPosting","@context":"https://schema.org"}</script><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Arthur Mensch"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="application-name" content="Arthur Mensch"><meta name="theme-color" content="#c4a071"><meta name="generator" content="Hydejack v9.1.4" /><link rel="alternate" href="/2016-06-10-massive-matrix-factorization-for-fMRI-and-recommender-systems/" hreflang="en"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Arthur Mensch" /><link rel="shortcut icon" href="/assets/icons/favicon.ico"><link rel="apple-touch-icon" href="/assets/icons/icon-192x192.png"><link rel="manifest" href="/assets/site.webmanifest"><link rel="dns-prefetch" href="https://fonts.googleapis.com"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preload" href="/assets/img/swipe.svg" as="image" id="_hrefSwipeSVG"> <script>!function(r,c){"use strict";function a(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}r.loadJS=function(e,t){var n=c.createElement("script");n.src=e,t&&a(n,"load",t,{once:!0});t=c.scripts[0];return t.parentNode.insertBefore(n,t),n},r._loaded=!1,r.loadJSDeferred=function(e,t){var n=c.createElement("script");function o(){r._loaded=!0,t&&a(n,"load",t,{once:!0});var e=c.scripts[0];e.parentNode.insertBefore(n,e)}return n.src=e,r._loaded?o():a(r,"load",o,{once:!0}),n},r.setRel=r.setRelStylesheet=function(e){a(c.getElementById(e),"load",function(){this.rel="stylesheet"},{once:!0})}}(window,document); !function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this); !function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this); !function(w) { w._baseURL = '/'; w._publicPath = '/assets/js/'; w._noPushState = false; w._noDrawer = false; w._noNavbar = false; w._noToc = false; w._noSearch = false; w._search = { DATA_URL: '/assets/sitedata.json?no-cache', STORAGE_KEY: 'mini-search/', INDEX_KEY: 'index--2021-10-03T12:58:56+02:00', }; w._clapButton = false; }(window);</script> <script async src="/assets/bower_components/MathJax/es5/tex-mml-chtml.js" id="_MathJax"></script> <!--[if gt IE 8]><!----><style id="_styleInline"> .clearfix,.sidebar-social::after{content:"";display:table;clear:both}.color-transition,.content .avatar,.nav-btn,.nav-btn-bar,.navbar,.message,.note-sm,#markdown-toc,.note,.hr-bottom,.hr-after::after,hr,.hr,p,body{transition:none}html{--font-family: Noto Sans,Helvetica,Arial,sans-serif;--font-family-heading: Roboto Slab,Helvetica,Arial,sans-serif;--code-font-family: Fira Code,Menlo,Monaco,Consolas,monospace;--body-color: #333;--body-bg: #fff;--border-color: #ebebeb;--gray: #777;--gray-bg: rgba(0,0,0,0.025);--gray-text: #666;--menu-text: #bbb;--inv-body-color: #ccc;--inv-body-bg: #282828;--root-font-size: 15px;--root-font-size-medium: 16px;--root-font-size-large: 17px;--root-font-size-print: 8pt;--root-line-height: 1.75;--font-weight: 400;--font-weight-bold: 700;--font-weight-heading: 700;--content-width-5: 54rem;--content-margin-5: 4rem;--sidebar-width: 21rem;--half-content: 31rem;--break-point-3: 64em;--break-point-5: 86em;--break-point-dynamic: 1664px}html .content{-webkit-font-smoothing:initial;-moz-osx-font-smoothing:initial}*{box-sizing:border-box}html,body{margin:0;padding:0}html{font-family:var(--font-family);font-size:var(--root-font-size);line-height:var(--root-line-height)}body{color:var(--body-color);background-color:var(--body-bg);font-weight:var(--font-weight);overflow-y:scroll}.content img,.img,.content video,.video{max-width:100%;height:auto}.lead{margin-left:-1rem;margin-right:-1rem;margin-bottom:1.5rem}img.lead,video.lead{display:block;max-width:calc(100% + 2rem);width:calc(100% + 2rem);height:auto}.heading,.f6,h6,.h6,.f5,h5,.h5,.f4,.sidebar-nav-item,h4,.h4,.post-date,.f3,h3,.h3,.f2,h2,.h2,.f1,h1,.h1{font-family:var(--font-family-heading);font-weight:var(--font-weight-heading)}.f1,h1,.h1{font-size:2rem;line-height:1.3}.f2,h2,.h2{font-size:1.5rem;line-height:1.4}.f3,h3,.h3{font-size:1.2em;line-height:1.5}.f4,.sidebar-nav-item,h4,.h4,.post-date{font-size:1.08rem;line-height:1.6}.f5,h5,.h5{font-size:1.04rem;line-height:1.7}.f6,h6,.h6{font-size:1rem}.content h1>a,.content .h1>a{text-decoration:none;border-bottom:none}@media screen and (max-width: 42em){.content h1,.content .h1{font-size:1.7rem;line-height:1.35}}@media screen and (min-width: 86em){.content h1,.content .h1{font-size:2.4rem;line-height:1.25}}@media screen and (min-width: 1664px){body:not(.no-large-headings) .content h1,body:not(.no-large-headings) .content .h1{width:calc(100% + 50vw - 32rem);font-size:3rem;line-height:1.2}}@media screen and (min-width: 124em){body:not(.no-large-headings) .content h1,body:not(.no-large-headings) .content .h1{font-size:4rem;line-height:1.1}}h1,h2,h3,.h1,.h2,.h3{margin:4rem 0 1rem}h4,h5,h6,.h4,.post-date,.h5,.h6{margin:3rem 0 .5rem}p{margin-top:0;margin-bottom:1rem}p.lead{font-size:1.2em;margin-top:1.5rem;margin-bottom:1.5rem;padding:0 1rem}ul,ol,dl{margin-top:0;margin-bottom:1rem}ul,ol{padding-left:1.25rem}hr,.hr{border:0;margin:1rem 0;border-top:1px solid var(--border-color)}.hr-after::after{content:"";display:block;margin:1rem 0;border-top:1px solid var(--border-color)}.hr-bottom{border-bottom:1px solid var(--border-color);padding-bottom:.75rem;margin-bottom:1rem}.page{margin-bottom:3em}.page li+li{margin-top:.25rem}.page>header{position:relative;margin-bottom:1.5rem}@media screen and (min-width: 1664px){body:not(.no-third-column) .page>header>.lead+.note-sm,body:not(.no-third-column) .page>header>.lead+#markdown-toc,body:not(.no-third-column) .page>header>.lead+.note,body:not(.no-third-column) .page>header>a.no-hover+.note-sm,body:not(.no-third-column) .page>header>a.no-hover+#markdown-toc,body:not(.no-third-column) .page>header>a.no-hover+.note{position:absolute;right:-25rem;width:21rem;bottom:0;margin-bottom:0}}.page-title,.post-title{margin-top:0}.post-date{display:flex;justify-content:space-between;margin-top:-.6rem;height:2rem;margin-bottom:.85rem;color:var(--gray)}.post-date>.ellipsis,#breadcrumbs.post-date>ul{cursor:pointer}.post-date [class^="icon-"]{display:inline-block;font-size:smaller;margin-right:.25rem}.related-posts{padding-left:0;list-style:none;margin-bottom:2rem}.related-posts>li,.related-posts>li+li{margin-top:1rem}.message,.note-sm,#markdown-toc,.note{margin-bottom:1rem;padding:1rem;color:var(--gray-text);background-color:var(--gray-bg);margin-left:-1rem;margin-right:-1rem}.note-sm,#markdown-toc,.note{background:transparent;color:var(--body-color);font-size:smaller;border-left:1px solid var(--border-color);padding:1.2rem 1rem 0 1rem;margin:1rem -1rem;position:relative}.note-sm:before,#markdown-toc:before,.note:before{font-size:0.667rem;font-weight:bold;font-style:normal;letter-spacing:.025rem;text-transform:uppercase;color:var(--menu-text);position:absolute;top:0}.note-sm[title]:before,#markdown-toc[title]:before,.note[title]:before{content:attr(title) !important}.note{font-size:1rem}@media screen{body::before{content:'';width:.5rem;background:var(--gray-bg);position:fixed;left:0;top:0;bottom:0}}@media (min-width: 64em){body::before{width:21rem}}@media (min-width: 1664px){body::before{width:calc(50% - 31rem)}}@media screen and (min-width: 42em){html{font-size:var(--root-font-size-medium)}}@media screen and (min-width: 124em){html{font-size:var(--root-font-size-large)}}#breadcrumbs>ul{height:1rem;margin:-1.5rem 0 .5rem;padding:0;font-size:.667rem;color:var(--menu-text);text-transform:uppercase;width:100%;list-style:none}#breadcrumbs>ul>li{display:inline}#breadcrumbs>ul>li a{color:var(--gray);text-decoration:none;border-bottom:none}.fl{float:left}.fr{float:right}.mb4{margin-bottom:4rem}.mb6{margin-bottom:6rem}.mt0{margin-top:0}.mt1{margin-top:1rem}.mt2{margin-top:2rem}.mt3{margin-top:3rem}.mt4{margin-top:4rem}.pb0{padding-bottom:0}.ml1{margin-left:1rem}.mr1{margin-right:1rem}.sixteen-nine{position:relative}.sixteen-nine::before{display:block;content:"";width:100%;padding-top:56.25%}.sixteen-nine>*{position:absolute;top:0;left:0;right:0;bottom:0}.sixteen-ten{position:relative}.sixteen-ten::before{display:block;content:"";width:100%;padding-top:62.5%}.sixteen-ten>*{position:absolute;top:0;left:0;right:0;bottom:0}.four-three{position:relative}.four-three::before{display:block;content:"";width:100%;padding-top:75%}.four-three>*{position:absolute;top:0;left:0;right:0;bottom:0}.sr-only{display:none}.larger{font-size:larger}.smaller{font-size:smaller}.clearfix,.sidebar-social::after{content:"";display:table;clear:both}.ellipsis,#breadcrumbs>ul{white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.border{border:1px solid var(--border-color)}@media (min-width: 42em){.border-radius,.lead,.page .aspect-ratio.sixteen-nine.lead{border-radius:.5rem}}.fallback-img{background-position:center;background-repeat:no-repeat;background-image:url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPHN2ZyB3aWR0aD0iMTYwIiBoZWlnaHQ9IjkwIiB2aWV3Qm94PSIwIDAgMTYwIDkwIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogIDxnIHRyYW5zZm9ybT0ibWF0cml4KDAuMDQ4ODI4LCAwLCAwLCAwLjA0Nzk5MSwgNTQuOTk5OTczLCAyMC40MjgxNDgpIj4KICAgIDxwYXRoIHN0eWxlPSJmaWxsOnJnYmEoMTI4LDEyOCwxMjgsLjMzKSIgZD0iTTk1OS44ODQgMTI4YzAuMDQwIDAuMDM0IDAuMDgyIDAuMDc2IDAuMTE2IDAuMTE2djc2Ny43N2MtMC4wMzQgMC4wNDAtMC4wNzYgMC4wODItMC4xMTYgMC4xMTZoLTg5NS43N2MtMC4wNDAtMC4wMzQtMC4wODItMC4wNzYtMC4xMTQtMC4xMTZ2LTc2Ny43NzJjMC4wMzQtMC4wNDAgMC4wNzYtMC4wODIgMC4xMTQtMC4xMTRoODk1Ljc3ek05NjAgNjRoLTg5NmMtMzUuMiAwLTY0IDI4LjgtNjQgNjR2NzY4YzAgMzUuMiAyOC44IDY0IDY0IDY0aDg5NmMzNS4yIDAgNjQtMjguOCA2NC02NHYtNzY4YzAtMzUuMi0yOC44LTY0LTY0LTY0djB6Ii8+CiAgICA8cGF0aCBzdHlsZT0iZmlsbDpyZ2JhKDEyOCwxMjgsMTI4LC4zMykiIGQ9Ik04MzIgMjg4YzAgNTMuMDIwLTQyLjk4IDk2LTk2IDk2cy05Ni00Mi45OC05Ni05NiA0Mi45OC05NiA5Ni05NiA5NiA0Mi45OCA5NiA5NnoiLz4KICAgIDxwYXRoIHN0eWxlPSJmaWxsOnJnYmEoMTI4LDEyOCwxMjgsLjMzKSIgZD0iTTg5NiA4MzJoLTc2OHYtMTI4bDIyNC0zODQgMjU2IDMyMGg2NGwyMjQtMTkyeiIvPgogIDwvZz4KPC9zdmc+")}hy-push-state a{color:var(--body-color)}@supports not ((text-decoration-thickness: initial) and (text-underline-offset: initial)){hy-push-state a{text-decoration:none;border-bottom:2px solid}}@supports (text-decoration-thickness: initial) and (text-underline-offset: initial){hy-push-state a{text-decoration-style:solid;text-underline-offset:.35rem;text-decoration-thickness:2px}}hy-push-state a.no-hover{border-bottom:none;text-decoration-thickness:unset;text-underline-offset:unset}.content a:not(.btn):not(.no-hover){border-color:var(--accent-color-faded)}@supports (text-decoration-thickness: initial) and (text-underline-offset: initial){.content a:not(.btn):not(.no-hover){text-decoration-color:var(--accent-color-faded)}}.content .aspect-ratio{overflow:hidden}.content .aspect-ratio img{margin:0;width:100%;height:100%;background-color:var(--gray-bg)}hy-drawer{width:100%;position:relative;overflow:hidden;display:block;z-index:4}@media screen and (min-width: 64em){hy-drawer{position:fixed;width:21rem;top:0;left:0;bottom:0;margin-left:0}hy-drawer.cover{position:relative;width:100%}}@media screen and (min-width: 1664px){hy-drawer{width:calc(50% - 31rem)}}.sidebar{position:relative;display:flex;justify-content:center;align-items:center;color:rgba(255,255,255,0.75);text-align:center;min-height:100vh}.sidebar.invert{color:rgba(32,32,32,0.75)}.sidebar a{color:#fff;border-bottom-color:rgba(255,255,255,0.2);text-decoration-color:rgba(255,255,255,0.2)}.sidebar.invert a{color:#222;border-bottom-color:rgba(32,32,32,0.2);text-decoration-color:rgba(32,32,32,0.2)}.sidebar-bg{position:absolute;top:0;left:calc(50% - 50vw);width:100vw;height:100%;background:#202020 center / cover}.sidebar-bg::after{content:"";position:absolute;top:0;left:0;bottom:0;right:0;background:rgba(0,0,0,0.05)}.sidebar-bg.sidebar-overlay::after{background:linear-gradient(to bottom, rgba(32,32,32,0) 0%, rgba(32,32,32,0.5) 50%, rgba(32,32,32,0) 100%)}.sidebar-sticky{position:relative;z-index:3;max-width:21rem;padding:1.5rem;contain:content}.sidebar-about .avatar{margin-bottom:1.5rem}.sidebar-about>a.sidebar-title{text-decoration:none}.sidebar-about>a.sidebar-title>h2{margin:0;padding-bottom:.5rem}.sidebar-about>a.sidebar-title::after{content:'';display:block;border-bottom:2px solid;margin:0 auto .5rem;width:4rem;border-color:rgba(255,255,255,0.2);transition:border-color 250ms}.sidebar-about>a.sidebar-title:hover::after{border-color:#fff;transition:border-color 50ms}.sidebar.invert .sidebar-about>a.sidebar-title::after{border-color:rgba(32,32,32,0.2)}.sidebar.invert .sidebar-about>a.sidebar-title:hover::after{border-color:#222}.sidebar-nav>ul{list-style:none;padding-left:0}.sidebar-nav-item{display:inline-block;margin-bottom:.5rem}@media (min-width: 64em){#_main.no-drawer #_menu{display:none}#_main.no-drawer .nav-btn-bar>:nth-child(2){border:none}}.sidebar-social>ul{display:inline-block;list-style:none;padding-left:0;margin-bottom:0}.sidebar-social>ul>li{float:left}.sidebar-social>ul>li>a{display:inline-block;text-align:center;font-size:1.4rem;width:3rem;height:4rem;padding:.5rem 0;line-height:3rem;text-decoration:none;border-bottom-width:2px;border-bottom-style:solid}.sidebar-social>ul li+li{margin-top:0}.fixed-common,.fixed-bottom,.fixed-top{position:fixed;left:0;width:100%;z-index:2}.fixed-top{top:0}.fixed-bottom{bottom:0}.navbar>.content{position:relative;padding-top:0;padding-bottom:0;min-height:0;max-height:5rem}.nav-btn-bar{margin:0 -1rem;background-color:white;background-color:var(--body-bg);height:5rem;display:flex;align-items:center;position:relative}.nav-btn-bar>:first-child,.nav-btn-bar>:last-child{border:none}.nav-btn{background:none;border:none;text-decoration:none;display:flex;align-items:center;justify-content:center;width:3.25rem;height:5rem;color:var(--menu-text);border-right:1px solid var(--border-color);border-left:1px solid var(--border-color);margin-left:-1px}#markdown-toc{margin:2rem -1rem 2rem calc(-1rem + 1px);padding-left:2.5rem;padding-bottom:.5rem}#markdown-toc:before{left:1rem}@media screen and (min-width: 1664px){body:not(.no-toc) #markdown-toc{position:absolute;z-index:4;width:20.5rem;right:0;margin:auto;overflow:auto}}@media screen and (min-width: 1664px){body.no-break-layout:not(.no-toc) #markdown-toc{width:calc(50% - 31rem)}}.content{margin-left:auto;margin-right:auto;padding:8rem 1rem 12rem}@media screen{.content{padding-left:1.5rem;min-height:100vh}}@media screen and (min-width: 42em){.content{max-width:42rem}}@media screen and (min-width: 54em){.content{max-width:48rem}}@media screen and (min-width: 64em){.content{padding-left:1rem;margin-left:24rem;margin-right:3rem}}@media screen and (min-width: 86em){.content{padding-top:9rem;margin-left:25rem;margin-right:4rem;max-width:54rem}}@media screen and (min-width: 1664px){.content{margin:auto}}.large-only{display:none}@media screen and (min-width: 1664px){.large-only{display:block}}.avatar{width:7rem;height:7rem;border-radius:100%;overflow:hidden;display:inline-block}.avatar img{width:100%}.content .avatar{float:right;box-sizing:content-box;border:1rem solid var(--body-bg);transition:border-color 1s ease;margin-top:-1.5rem;margin-right:-1rem}.note:before{content:"Note"}.page>header>.note-sm:before,.page>header>.note:before,.page>header>#markdown-toc:before{content:"Description"}#markdown-toc:before{content:"Table of Contents"}.layout-resume .note-sm:before,.layout-resume .note:before,.layout-resume #markdown-toc:before{content:"Summary"}</style><link rel="preload" as="style" href="/assets/css/hydejack-9.1.4.css" id="_stylePreload"><link rel="preload" as="style" href="/assets/icomoon/style.css" id="_iconsPreload"><link rel="preload" as="style" href="https://fonts.googleapis.com/css?family=Roboto+Slab:700%7CNoto+Sans:400,400i,700,700i&display=swap" id="_fontsPreload"> <script> setRel('_stylePreload'); setRel('_iconsPreload'); /**/setRel('_fontsPreload');/**/ </script> <noscript><link rel="stylesheet" href="/assets/css/hydejack-9.1.4.css"><link rel="stylesheet" href="/assets/icomoon/style.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:700%7CNoto+Sans:400,400i,700,700i&display=swap"> </noscript><style id="_pageStyle"> html{--accent-color: #c4a071;--accent-color-faded: rgba(196,160,113,0.5);--accent-color-highlight: rgba(196,160,113,0.1);--accent-color-darkened: #b98e56;--theme-color: #c4a071}</style><!--<![endif]--><body class="no-break-layout no-large-headings"> <hy-push-state id="_pushState" replace-selector="#_main" link-selector="a[href]:not([href^='/assets/']):not(.external):not(.no-push-state)" script-selector="script" duration="500" hashchange ><div id="_navbar" class="navbar fixed-top"><div class="content"> <span class="sr-only">Jump to:</span><div class="nav-btn-bar"> <a id="_menu" class="nav-btn no-hover" href="#_drawer--opened"> <span class="sr-only">Navigation</span> <span class="icon-menu"></span> </a><div class="nav-span"></div></div></div></div><hr class="sr-only" hidden /><main id="_main" class="content layout-post" role="main" ><nav id="breadcrumbs" class="screen-only"><ul><li><a href="/">home</a><li> <span>/</span> <span>2016-06-10-massive-matrix-factorization-for-fMRI-and-recommender-systems</span></ul></nav><article id="post-massive-matrix-factorization-for-fMRI-and-recommender-systems" class="page post mb6" role="article"><header><h1 class="post-title flip-project-title"> Massive Matrix Factorization for fMRI and Recommender Systems</h1><div class="post-date"> <span class="ellipsis mr1"> <time datetime="2016-06-10T10:19:34+02:00">10 Jun 2016</time> on <span>Optimization</span>, <span>Matrix factorization</span> </span> <span class="ellipsis" data-tippy-content="Last modified at 2018-03-19"> <span class="sr-only">Last modified at</span> <span class="icon-history"></span> <time datetime="2018-03-19T11:19:53+01:00">2018-03-19</time> </span></div><div class="hr pb0"></div></header><p>Before presenting it at ICML New York, I will give a quick overview of our work on efficient matrix factorization for very large datasets. Our focus was to scale <em>matrix factorization</em> techniques for functional MRI, a domain where data to decompose is now at terabyte scale. Along the way, we also designed a encouraging proof-of-concept experiment for collaborative filtering.<p>We’ll start by reviewing matrix factorization techniques for interpretable data decomposition.<h2 id="understanding-data-with-matrix-factorization">Understanding data with matrix factorization</h2><p>Unsupervised learning aim at finding patterns in a sequence of n samples \((x_t)t\), living in a \(p\) dimensional space. Typically, this involve finding a few statistics that describe data in a <em>compressed</em> manner. Our dataset can be seen as a large matrix \(X \in R^{n \times p}\). Factorizing such matrix has proven a very flexible manner to extract interesting pattern. Namely, we want to find two <em>small</em> matrices \(D\) (the <em>dictionary</em>) and \(A\) (the <em>code</em>) with \(k\) columns/rows whose product approximates \(X\)<p><img src="/assets/img/16-mmf/drawings/poster_model_sparse.png" width="80%" style="display: block; margin: 0 auto;" title="Model" /><p>Small can mean several things here : we may impose \(k\) to be small, which amounts to search for a low rank representation of the matrix \(X\), and thus a subspace of \(RR^p\) that approximately include all samples. For interpretability, it can be useful, as in the drawing above, to impose sparsity on \(D\) – this is what we’ll do in fMRI.<p>In other settings, we may have \(k\) large but impose \(A\) <em>sparse</em>, leading to an overcomplete dictionary learning setting, that generalize the k-means algorithm. This setting won’t interest us today, although we use its terminology.<h3 id="fmri-example">fMRI example</h3><p>We can already instantiate matrix factorization for fMRI as this will make things clearer. We study resting-state functional imaging : 500 subjects go four times in a scanner, to get their brain activity recorded during 15 minutes while at rest – roughly, a 3D image of their brain activity is acquired every second. This yields 2 millions 3D images of brain activity, each of them with 200 000 <em>voxels</em> – <strong>2TB</strong> of dense data. We want to extract spatial activity maps that constitute a good basis for these images:<p><img src="/assets/img/16-mmf/drawings/poster_fmri_dl_flat.png" width="80%" style="display: block; margin: 0 auto;" title="Model" /><p>What we are most interested in is the dictionary \(D\), that holds, say, 70 sparse spatial maps. We expect those to capture functional networks, segmenting the auditory, visual, motor cortex, etc. Sparsity and low-rank are key for pattern discovery: we want to find few maps, with few activated regions.<h2 id="matrix-factorization-for-large-datasets">Matrix factorization for large datasets</h2><p>A little math should be introduced to better grasp our problem. Decomposing \(X\) into the product \(D A\) can be done by solving an optimization problem (see <strong>[Olshausen ‘97]</strong> for the initial problem setting): \[\min_{D \in \mathcal{C}, A \in R^{k\times p}} \Vert X - D A \Vert_2^2 + \lambda \Omega(\alpha)\]<p>where structure and sparsity can be imposed via constraints (convex set \(\mathcal{C}\)) and penalties. For example, we may impose dictionary columns to live in \(\ell_1\) balls, to get a sparse dictionary.<p>Solving this minimization problem is where all the honey is : let’s see what methods can be used when \(X\) grows large.<p>A naive solver alternatively minimize the loss function over \(A\) and \(D\). Meaning, given \(X\) and \(A\), find the best \(D\), given \(X\) and \(D\), find the best \(A\), and repeat. If we look at it from a dictionary oriented point of view, we define \(A(D) = \text{arg}\,\min_{A \in R^{k \times n}} \Vert X - D A \Vert_F^2 + \lambda \Omega(A)\) \[\alpha_i(D) = \text{arg}\,\min_{A \in R^{k \times n}} \Vert x_i - D \alpha_i \Vert_F^2 + \lambda \Omega(\alpha_i)\]<p>where the second equality has used the colummns \((\alpha_i)\) of \(A\) – we’ll see why in a minute. The naive algorithm simply consist in doing \[\begin{aligned} D_t &amp;= \text{arg}\,\min_{D \in \mathcal{C}} \Vert X - D A(D_{t-1}) \Vert_F^2 \\ &amp;= \min_{D} \sum_{i=1}^n \Vert x_i - D \alpha_i(D_{n-1})) \Vert_F^2 \end{aligned}\]<p>This takes time, as the whole data \(X\) is loaded at each iteration. In fact, it quickly becomes intractable: beyond 1 million entry in \(X\), it already takes hours.<h3 id="going-online">Going online</h3><p>A very efficient way to get past this intractability was introduced by <strong>[Mairal ‘10]</strong>. Computing \(A\) for the whole dataset is costly, and overkill for a single step of improving the dictionary: we can maintain an approximation of this code by streaming the data and optimizing the dictionary along the stream.<p><img src="/assets/img/16-mmf/drawings/poster_model_sparse_online.png" width="80%" style="display: block; margin: 0 auto;" title="Model" /><p>As the drawing above indicates, we look at data sample \(x_t\) after sample. At iteration \(t\)t, we use the current dictionary to compute the associated loadings \(\alpha_t\): \[\alpha_t(D) = \text{arg}\,\min_{A \in R^{k \times n}} \Vert x_t - D_{t-1} \alpha_t \Vert_F^2 + \lambda \Omega(\alpha_t)\]<p>We then solve, at each iteration \[D_t = \text{arg}\,\min_{D \in \mathcal{C}} \sum_{i=1}^t \Vert x_i - D \alpha_i \Vert_F^2\]<p>This look very much like the original update, except we use outdated \(\alpha_t\) to approximate our objective function. The essential idea here is start solving the problem with a very inaccurate approximation of it, and improve it by looking at more data.<p>A single iteration of the algorithm depend on \(p\) but no longer on \(n\), and the algorithm empirically converges in a few epochs on data. This is very efficient when data dimension \(p\) is reasonable – as a matter of fact the online algorithm was initially designed to handle large sequences of 16x16 image patches – <strong>a very low p compared to fMRI setting</strong>.<h2 id="handling-large-sample-dimension">Handling large sample dimension</h2><p>This is where our contribution begins. We want to provide an algorithm that scales not only in the number of samples but also in the sample dimension. To scale in the number of samples, we went from using \(X\) to using \(x_t\) at each iteration, allowing around n time faster iterations. Here, \(x_t\) is still too large, and <strong>we want to acquire information even faster</strong>.<p>This is where we introduce <em>random subsampling</em>: can we improve the dictionary with only a <em>fraction</em> of a sample at each iteration. The answer is yes, as we’ll now show. The algorithm we propose loads a fraction of a sample \(x_t\) at each iteration and use it to update the approximation of the optimization problem. The fraction is different at each iteration: this way, we are able to obtain information about the whole feature space, in a stochastic manner. We go a step beyond in randomness:<p><img src="/assets/img/16-mmf/drawings/poster_next_level.png" alt="Random subsampling" /><p>\(M_t x_t\) corresponds to a subsampling of \(x_t\), choosing \(M_t\) to be a \([0, 1]\) diagonal matrix with, say, 90% zeros.<p>The whole difficulty lies in constructing the right approximations so that the problem we solve at each iteration looks more and more like the original optimization problem – just like the online algorithm does.<p>The online algorithm relies on a few low dimensional statistics that sufficiently describe the approximate problem. These are updated in a \(\mathcal{O}(p)\) cost – ensuring scalable single iteration, and hence the online magic.<p>Our objective here is to speed up iteration of a constant factor, that corresponds to the factor of dimension reduction. We must therefore ensure that everything we do at iteration t scales in \(\mathcal{O}(s)\), where \(s\) is the <em>reduced</em> dimension. That way, we gain a constant factor (from 2 to 12 on large datasets, as we’ll see) on single iteration complexity (<em>computational speed-up</em>), and we expect not to loose it because of the approximation we introduce (<em>approximation errance</em>).<p>This is because <strong>very large datasets have often many redundancies</strong>, accessing a stochastic part of sample does not reduce much the information acquired at each iteration. As we’ll see, on large datasets, the balance is therefore very much on the side of single iteration computational speed-up.<p>The constraint we introduce on iteration complexity restrains much what we are able to do. To sum up, we have to adapt the three steps of the online algorithm<ul><li>Computing the code from past iterate : we rely on a <em>sketched</em> version of code computation, where we only look at \(M_t\) features of \(x_t\) and \(D_{t-1}\)</ul>\[\begin{aligned}\alpha_t(D) &amp;= \text{arg}\,\min_{A \in R^{k \times n}} \Vert M_t(x_t - D_{t-1} \alpha_t) \Vert_F^2 + \lambda \frac{s}{p} \Omega(\alpha) \end{aligned}\]<ul><li><p>Aggregating this partial sample and code in an approximative objective, as we do by summing \(t\) factors in the online algorithm. We have to do this in a clever manner so that we only update statistics of size in s and not in p. This includes keeping tracks of the number of time we saw a feature in the past.<li><p>Updating the dictionary: we can’t update the full \(D\) at each iteration as this is \(\mathcal{O}(p)\) costly. It makes sense to update the features of the dictionary atoms that were seen in \(M_t\), ensuring that \(D\) remains in \(\mathcal{C}\) by projection.</ul><p>I skipped the math in the two last parts, but you can access it in more detail <a href="docs/presentations/icml_presentation.pdf">on these slides</a>. You will also find a detailed comparison between our algorithm and the original online algorithm.<h2 id="results">Results</h2><p>Let’s get to the most important part: do we get desired speed-up, is the dictionary we compute as good as those we would obtain with previous algorithms ?<p><strong>On fMRI, we can push the reduction up to x12 and obtain x10 speed-up compared to the online algorithm</strong>. Remember that a single pass on the data would take 235h using the online algorithm. We’ll use the obtained maps as a baseline. Maps are blobish, with noiseless contours.<p>In no more than 10h, our algorithm, using a 12-fold reduction, is able to recover maps that are almost as epxloitable as the baseline one. In comparison, the original algorithm stopped after 10h yields very poor results: noisy maps with many blobs.<p>Displaying the contour of these maps makes it clearly appear:<p><img src="/assets/img/16-mmf/figures/brains.png" alt="Brains" /><p>We can quantify the speed-up we obtain by looking at convergence curve, that decribe how good the dictionary perform as a basis on a test set, against time spent in computation.<p><img src="/assets/img/16-mmf/figures/bench.png" width="70%" style="display: block; margin: 0 auto;" title="Bench" /><p><strong>Convergence is obtained x10 more quickly</strong> with a 12 times reduction. This is very valuable for practioners ! Information is indeed acqired faster, as the speed-up we obtained is close to the reduction we imposed.<h2 id="collaborative-filtering">Collaborative filtering</h2><p>Our setting imposes masks on data to speed up learning. Quite interestingly, collaborative filtering brings us a setting where we can only acces <em>masked</em> data, that corresponds to, for example, the few movies that a user has rated. Matrix factorization is there used to reconstructe the incomplete matrix \(X\) (see, for instance <strong>[Szabo ‘11’]</strong>). To evaluate the performance of our model, we look at rating prediction on a test set. We compare our algorithm with a fast coordiate descent solver from <a href="http://github.com/mathieublondel/spira">spira</a>, that does not involve setting any hyperparmeter – our algorithm is, unlike SGD, not very dependant on hyperparameters. We get good results on large datasets (Netflix, Movielens 10M), as these benches show. On <strong>Netflix</strong>, our algorithm is <strong>7x faster</strong> than the coordinate descent solver, which was the fastest well-packaged collaborative filtering algorithm we could find.<p><img src="/assets/img/16-mmf/figures/rec_bench.png" width="100%" style="display: block; margin: 0 auto;" title="Collaborative filtering benches" /><p>Our model is very simple (minimization of an \(\ell_2\) loss), and we do not get state of the art prediction on Netflix. However, this experiment shows that our algorithm is able to learn a decomposition even with non random masks, and demonstrate the efficiency of imposing the complexity constraints explained above.<h2 id="conclusion">Conclusion</h2><p>Leveraging random subsampling with online learning is thus a very efficient manner to perform matrix factorization on datasets large in both direction. Our algorithm had no convergence guarantee at the time of contribution (February), but we now have a slightly adapted algorithm that converges with the same guarantee as in the original online matrix factorization paper – we rely on the stochastic majorization minimization framework <strong>[Mairal ‘13]</strong>.<p><a href="http://github.com/arthurmensch/modl">A Python package</a> is available for reproducibility. We hope to integrate this algorithm in more well-known library in the long term.<p>I hope that this post was readable enough and has interested you. You’ll find more details in our <a href="https://hal.archives-ouvertes.fr/hal-01308934">paper</a>, <a href="/docs/posters/icml_poster.pdf">poster</a> and <a href="/docs/presentations/icml_presentation.pdf">slides</a>. I will present this work in ICML New York Monday June 20th at 11h45. Discussions are welcome !<h2 id="references">References</h2><ul><li><p><strong>[Mairal ‘10]</strong> Mairal, Julien, Francis Bach, Jean Ponce, and Guillermo Sapiro. “Online Learning for Matrix Factorization and Sparse Coding.” The Journal of Machine Learning Research, 2010.<li><p><strong>[Mairal ‘13]</strong> Mairal, Julien. “Stochastic Majorization-\minimization Algorithms for Large-Scale Optimization.” In Advances in Neural Information Processing Systems, 2013.<li><p><strong>[Olshausen ‘97]</strong> Olshausen, Bruno A., and David J. Field. “Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1?” Vision Research, 1997.<li><p><strong>[Szabo ‘11]</strong> Szabó, Zoltán, Barnabás Póczos, and András Lorincz. “Online Group-Structured Dictionary Learning.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011.<li><p>See also <a href="/docs/presentations/icml_presentation.pdf">these slides</a></ul></article><hr class="dingbat related mb6" /><aside class="about related mt4 mb4" role="complementary"><div class="author mt4"> <img src="/assets/img/amensch.jpg" alt="Arthur Mensch" class="avatar" width="120" height="120" loading="lazy" /><h2 class="page-title hr-bottom"> About</h2><p>I am a research scientist at <a href="https://deepmind.com">DeepMind</a>, that I have joined at the end of 2020. I work in Paris office.<p>I was a post-doctoral research at École Normale Supérieure, Paris, in <a href="https://gpeyre.com">Gabriel Peyré</a>’s lab. I hold a Ph.D. in machine learning, prepared in <a href="https://team.inria.fr/parietal/">Inria Parietal</a>, from 2015 to 2018.<p>I am currently interested in optimization and large-scale deep-learning, and continue to have interest for structured prediction, optimal transport and game theory.<p>My Ph.D. was obtained under the supervision of <a href="http://gael-varoquaux.info">Gaël Varoquaux</a>, <a href="http://lear.inrialpes.fr/people/mairal/">Julien Mairal</a> and <a href="https://team.inria.fr/parietal/team-members/bertrand-thirions-page/">Bertrand Thirion</a>. I developed new stochastic algorithms and multi-task models for terabyte sized fMRI dataset analysis.<p>My detailed resume can be found <a href="docs/cv.pdf">here</a>. More details on my <a href="research">research</a> and on <a href="software">software</a>.<div class="sidebar-social"> <span class="sr-only">Social</span><ul><li> <a href="https://twitter.com/arthurmensch" title="Twitter" class="no-mark-external"> <span class="icon-twitter"></span> <span class="sr-only">Twitter</span> </a><li> <a href="https://github.com/arthurmensch" title="GitHub" class="no-mark-external"> <span class="icon-github"></span> <span class="sr-only">GitHub</span> </a><li> <a href="mailto:arthur.mensch@m4x.org" title="Email" class="no-mark-external"> <span class="icon-mail"></span> <span class="sr-only">Email</span> </a><li> <a href="https://amensch.fr/feed.xml" title="rss" class="no-mark-external"> <span class="icon-link"></span> <span class="sr-only">rss</span> </a></ul></div></div></aside><footer role="contentinfo"><hr/><p><small class="copyright">© 2021. All rights reserved. </small><nav class="legal"><small> <a class="heading flip-title" href="/LICENSE/">LICENSE</a> </small></nav><hr class="sr-only"/></footer></main><hy-drawer id="_drawer" class="" side="left" threshold="10" noscroll ><header id="_sidebar" class="sidebar" role="banner"><div class="sidebar-bg sidebar-overlay" style="background-color:#c4a071;background-image:url(/assets/img/agrigente-small.jpg)"></div><div class="sidebar-sticky"><div class="sidebar-about"> <a class="sidebar-title" href="/"><h2 class="h1">Arthur Mensch</h2></a><p class=""> Machine learning / Optimization</div><nav class="sidebar-nav heading" role="navigation"> <span class="sr-only">Navigation:</span><ul><li> <a id="_navigation" href="/" class="sidebar-nav-item" > Welcome </a><li> <a href="/research/" class="sidebar-nav-item" > Selected research </a><li> <a href="/software/" class="sidebar-nav-item" > Software </a><li> <a href="/teaching/" class="sidebar-nav-item" > Teaching </a></ul></nav><div class="sidebar-social"> <span class="sr-only">Social</span><ul><li> <a href="https://twitter.com/arthurmensch" title="Twitter" class="no-mark-external"> <span class="icon-twitter"></span> <span class="sr-only">Twitter</span> </a><li> <a href="https://github.com/arthurmensch" title="GitHub" class="no-mark-external"> <span class="icon-github"></span> <span class="sr-only">GitHub</span> </a><li> <a href="mailto:arthur.mensch@m4x.org" title="Email" class="no-mark-external"> <span class="icon-mail"></span> <span class="sr-only">Email</span> </a><li> <a href="https://amensch.fr/feed.xml" title="rss" class="no-mark-external"> <span class="icon-link"></span> <span class="sr-only">rss</span> </a></ul></div></div></header></hy-drawer><hr class="sr-only" hidden /> </hy-push-state> <!--[if gt IE 10]><!----> <script nomodule>!function(){var t,n=document.createElement("script");!("noModule"in n)&&"onbeforeload"in n&&(t=!1,document.addEventListener("beforeload",function(e){if(e.target===n)t=!0;else if(!e.target.hasAttribute("nomodule")||!t)return;e.preventDefault()},!0),n.type="module",n.src=".",document.head.appendChild(n),n.remove())}(); </script> <script src="/assets/js/hydejack-9.1.4.js" type="module"></script> <script src="/assets/js/LEGACY-hydejack-9.1.4.js" nomodule defer></script> <!--<![endif]--><div hidden><h2 class="sr-only">Templates:</h2><clap-config> <clap-text at="1">Keep going!</clap-text> <clap-text at="2">Keep going ×2!</clap-text> <clap-text at="3">Give me more!</clap-text> <clap-text at="5">Thank you, thank you</clap-text> <clap-text at="7">Far too kind!</clap-text> <clap-text at="10">Never gonna give me up?</clap-text> <clap-text at="14">Never gonna let me down?</clap-text> <clap-text at="20">Turn around and desert me!</clap-text> <clap-text at="30">You're an addict!</clap-text> <clap-text at="40">Son of a clapper!</clap-text> <clap-text at="50">No way</clap-text> <clap-text at="60">Go back to work!</clap-text> <clap-text at="70">This is getting out of <em>hand</em></clap-text> <clap-text at="80">Unbelievable</clap-text> <clap-text at="90">PREPOSTEROUS</clap-text> <clap-text at="100">I N S A N I T Y</clap-text> <clap-text at="185"><span style="font-family:monospace">FEED ME A STRAY CAT</span></clap-text> </clap-config> <template id="_animation-template"><div class="animation-main fixed-top"><nav id="breadcrumbs" class="screen-only"><ul></ul></nav><div class="content"><div class="page"></div></div></div></template> <template id="_loading-template"><div class="loading nav-btn fr"> <span class="sr-only">Loading…</span> <span class="icon-cog"></span></div></template> <template id="_error-template"><div class="page"><h1 class="page-title">Error</h1><p class="lead"> Sorry, an error occurred while loading: <a class="this-link" href=""></a>.</div></template> <template id="_permalink-template"> <a href="#" class="permalink"> <span class="sr-only">Permalink</span> <span class="content-hash"></span> </a> </template></div></html>
